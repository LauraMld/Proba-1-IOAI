# -*- coding: utf-8 -*-
"""Spaceship3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EQjkvVghbmzSY89W2XyGgsF_lCx5rAhA
"""

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder

# Load data from Kaggle
from google.colab import files
files.upload()  # Upload your Kaggle API key file (kaggle.json)

!pip install -q kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle competitions download -c spaceship-titanic

# Unzip the dataset
!unzip spaceship-titanic.zip

# Load the data into pandas dataframes
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

# Data exploration and preprocessing
def preprocess_data(df):
    # Fill missing values
    df['Age'].fillna(df['Age'].median(), inplace=True)
    df['RoomService'].fillna(0, inplace=True)
    df['FoodCourt'].fillna(0, inplace=True)
    df['ShoppingMall'].fillna(0, inplace=True)
    df['Spa'].fillna(0, inplace=True)
    df['VRDeck'].fillna(0, inplace=True)
    df['CryoSleep'].fillna(False, inplace=True)
    df['VIP'].fillna(False, inplace=True)

    # Feature engineering
    df['FamilySize'] = df['PassengerId'].apply(lambda x: x.split('_')[0]).astype(int)

    # Extract Deck and Side from Cabin
    df['Deck'] = df['Cabin'].apply(lambda x: x.split('/')[0] if pd.notna(x) else 'Unknown')
    df['Side'] = df['Cabin'].apply(lambda x: x.split('/')[-1] if pd.notna(x) else 'Unknown')

    # Convert categorical variables to dummy variables
    df = pd.get_dummies(df, columns=['HomePlanet', 'Destination', 'Deck', 'Side'], drop_first=True)

    # Encode boolean columns
    df['CryoSleep'] = df['CryoSleep'].astype(int)
    df['VIP'] = df['VIP'].astype(int)

    # Drop unnecessary columns
    df.drop(['Name', 'Cabin', 'PassengerId'], axis=1, inplace=True)

    return df

train_df = preprocess_data(train_df)
test_df = preprocess_data(test_df)

# Splitting data into features and target variable
X = train_df.drop('Transported', axis=1)
y = train_df['Transported']

# Encode target variable
le = LabelEncoder()
y = le.fit_transform(y)

# Splitting the training data for validation
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest model
rf_model = RandomForestClassifier(random_state=42)

# Hyperparameter tuning with GridSearchCV
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# Get the best model
best_rf_model = grid_search.best_estimator_

# Validate the model
y_val_pred = best_rf_model.predict(X_val)
accuracy = accuracy_score(y_val, y_val_pred)
print(f'Validation Accuracy: {accuracy:.2f}')

# Train on the full training set
best_rf_model.fit(X, y)

# Make predictions on the test set
test_predictions = best_rf_model.predict(test_df)

# Prepare the submission file
submission = pd.read_csv('sample_submission.csv')
submission['Transported'] = test_predictions
submission['Transported'] = submission['Transported'].astype(bool)
submission.to_csv('submission.csv', index=False)

# Download the submission file
files.download('submission.csv')