# -*- coding: utf-8 -*-
"""DisasterTweets.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WmHGVO-rfDg7TCBz51xbOoC-ZHj90ur2
"""

from google.colab import files
files.upload()

import os
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle competitions download -c nlp-getting-started
!unzip nlp-getting-started.zip -d nlp

import nltk
nltk.download('stopwords')
nltk.download('wordnet')

import pandas as pd
import numpy as np
import re
import string
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
from tensorflow.keras.models import Model
from transformers import RobertaTokenizer, TFRobertaModel

# Load data
train_df = pd.read_csv("nlp/train.csv")
test_df = pd.read_csv("nlp/test.csv")

# Text preprocessing function
def clean_text(text):
    text = text.lower()
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    return text

train_df['text'] = train_df['text'].apply(clean_text)
test_df['text'] = test_df['text'].apply(clean_text)

# Load RoBERTa tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-large')

# Tokenize and pad sequences
max_length = 128
def roberta_encode(texts, tokenizer, max_length):
    enc_di = tokenizer.batch_encode_plus(
        texts,
        max_length=max_length,
        padding=True,
        truncation=True,
        return_attention_mask=True,
        return_tensors='tf'
    )
    return enc_di['input_ids'], enc_di['attention_mask']

X_train_ids, X_train_mask = roberta_encode(train_df['text'].tolist(), tokenizer, max_length)
X_test_ids, X_test_mask = roberta_encode(test_df['text'].tolist(), tokenizer, max_length)
y_train = train_df['target'].values

# Load RoBERTa model
roberta_model = TFRobertaModel.from_pretrained('roberta-large')

# Build model
input_ids = Input(shape=(max_length,), dtype=tf.int32, name="input_ids")
attention_mask = Input(shape=(max_length,), dtype=tf.int32, name="attention_mask")

roberta_output = roberta_model(input_ids, attention_mask=attention_mask)[0]
cls_token = roberta_output[:, 0, :]

dropout = Dropout(0.3)(cls_token)
output = Dense(1, activation='sigmoid')(dropout)

model = Model(inputs=[input_ids, attention_mask], outputs=output)

model.compile(Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])

# Callbacks
checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1)
early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)

# Train-validation split
X_train_split, X_val, y_train_split, y_val = train_test_split(
    {'input_ids': X_train_ids, 'attention_mask': X_train_mask},
    y_train,
    test_size=0.2,
    random_state=42
)

# Training
history = model.fit(
    X_train_split,
    y_train_split,
    validation_data=(X_val, y_val),
    epochs=5,
    batch_size=16,
    callbacks=[checkpoint, reduce_lr, early_stopping]
)

# Load the best model
model.load_weights('model.h5')

# Predict on test set
predictions = model.predict({'input_ids': X_test_ids, 'attention_mask': X_test_mask})
predictions = (predictions > 0.5).astype(int).flatten()

# Prepare submission
submission = pd.read_csv('nlp/sample_submission.csv')
submission['target'] = predictions
submission.to_csv('submission.csv', index=False)

#0.81305

import pandas as pd
import numpy as np
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Bidirectional, GlobalMaxPool1D, BatchNormalization, Dropout, Dense
from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
import re
import string
import os

# Load data
train_df = pd.read_csv("nlp/train.csv")
test_df = pd.read_csv("nlp/test.csv")

# Clean text function
def clean_text(text):
    text = text.lower()
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    return text

# Apply cleaning
train_df['text'] = train_df['text'].apply(lambda x: clean_text(x))
test_df['text'] = test_df['text'].apply(lambda x: clean_text(x))

# Tokenization and padding
tokenizer = Tokenizer()
tokenizer.fit_on_texts(train_df['text'])
vocab_size = len(tokenizer.word_index) + 1

max_length = max(train_df['text'].apply(lambda x: len(x.split())))
X_train = pad_sequences(tokenizer.texts_to_sequences(train_df['text']), maxlen=max_length, padding='post')
X_test = pad_sequences(tokenizer.texts_to_sequences(test_df['text']), maxlen=max_length, padding='post')
y_train = train_df['target'].values


# Load pre-trained GloVe embeddings
embedding_dim = 100
embedding_matrix = np.zeros((vocab_size, embedding_dim))
with open('glove.6B.100d.txt', encoding='utf8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        vector = np.asarray(values[1:], dtype='float32')
        if word in tokenizer.word_index:
            embedding_matrix[tokenizer.word_index[word]] = vector

# Model
model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))
model.add(Bidirectional(LSTM(64, return_sequences=True, recurrent_dropout=0.2)))
model.add(GlobalMaxPool1D())
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])

# Callbacks
checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=1)

# Train-Validation Split
X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Training
history = model.fit(X_train_split, y_train_split, epochs=10, batch_size=64, validation_data=(X_val, y_val), callbacks=[checkpoint, reduce_lr])

# Load the best model
model.load_weights('model.h5')

# Predict on test set
predictions = model.predict(X_test)

# Prepare submission
submission = pd.read_csv('nlp/sample_submission.csv')
submission['target'] = (predictions > 0.5).astype(int)
submission.to_csv('submission.csv', index=False)

# Prepare submission
submission = pd.read_csv('nlp/sample_submission.csv')
submission['target'] = (predictions > 0.5).astype(int)
submission.to_csv('submission.csv', index=False)

files.download('submission.csv')

import pandas as pd
import numpy as np
import re
import string
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.pipeline import Pipeline
import nltk

# Descărcarea resurselor necesare NLTK
nltk.download('stopwords')
nltk.download('wordnet')


train_data = pd.read_csv('nlp/train.csv')
test_data = pd.read_csv('nlp/test.csv')
print(train_data.head())

# Funcție pentru curățarea textului
def clean_text(text):
    text = text.lower()
    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'<.*?>+', '', text)
    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub(r'\n', '', text)
    text = re.sub(r'\w*\d\w*', '', text)
    text = re.sub(r'@\w+', '', text)  # Eliminarea mențiunilor (@user)
    text = re.sub(r'#\w+', '', text)  # Eliminarea hashtag-urilor (#hashtag)
    return text

# Aplicarea curățării textului
train_data['text'] = train_data['text'].apply(clean_text)
test_data['text'] = test_data['text'].apply(clean_text)

# Împărțirea datelor de antrenament în set de antrenament și set de validare
X_train, X_val, y_train, y_val = train_test_split(train_data['text'], train_data['target'], test_size=0.2, random_state=42)

# Adăugarea stopword-urilor și lematizării
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def lemmatize_text(text):
    words = text.split()
    lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]
    return ' '.join(lemmatized_words)

X_train = X_train.apply(lemmatize_text)
X_val = X_val.apply(lemmatize_text)
test_data['text'] = test_data['text'].apply(lemmatize_text)

# Crearea pipeline-ului pentru vectorizare și antrenare
pipeline = Pipeline([
    ('vectorizer', TfidfVectorizer(max_features=5000)),
    ('classifier', MultinomialNB())
])

# Ajustarea hiperparametrilor folosind GridSearchCV
param_grid = {
    'vectorizer__ngram_range': [(1, 1), (1, 2)],
    'classifier__alpha': [0.1, 1.0, 10.0]
}

grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

print(f'Best parameters: {grid_search.best_params_}')
print(f'Best cross-validation accuracy: {grid_search.best_score_}')

# Evaluarea modelului pe setul de validare
best_model = grid_search.best_estimator_
y_val_pred = best_model.predict(X_val)

accuracy = accuracy_score(y_val, y_val_pred)
report = classification_report(y_val, y_val_pred)

print(f'Acuratețea pe setul de validare: {accuracy * 100:.2f}%')
print('Raport de clasificare pe setul de validare:')
print(report)

# Generarea predicțiilor pentru setul de testare
X_test_vect = best_model.named_steps['vectorizer'].transform(test_data['text'])
test_predictions = best_model.named_steps['classifier'].predict(X_test_vect)

# Crearea DataFrame-ului pentru predicții
submission = pd.DataFrame({'id': test_data['id'], 'target': test_predictions})

# Salvarea predicțiilor într-un fișier CSV
submission.to_csv('submission.csv', index=False)

!pip install accelerate -U

pip install transformers[torch]

!pip install requests==2.21.0

import pandas as pd
import numpy as np
import re
import string
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
from transformers import pipeline

# Încărcarea setului de date

train_data = pd.read_csv('nlp/train.csv')
test_data = pd.read_csv('nlp/test.csv')

# Previzualizarea datelor
print(train_data.head())

# Funcție pentru curățarea textului
def clean_text(text):
    text = text.lower()
    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'<.*?>+', '', text)
    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub(r'\n', '', text)
    text = re.sub(r'\w*\d\w*', '', text)
    text = re.sub(r'@\w+', '', text)  # Eliminarea mențiunilor (@user)
    text = re.sub(r'#\w+', '', text)  # Eliminarea hashtag-urilor (#hashtag)
    return text

# Aplicarea curățării textului
train_data['text'] = train_data['text'].apply(clean_text)
test_data['text'] = test_data['text'].apply(clean_text)

# Împărțirea datelor de antrenament în set de antrenament și set de validare
X_train, X_val, y_train, y_val = train_test_split(train_data['text'], train_data['target'], test_size=0.2, random_state=42)

# Tokenizare și pregătirea datelor pentru BERT
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

def tokenize_data(texts, labels):
    tokenized_data = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors="pt")
    tokenized_data['labels'] = torch.tensor(labels)
    return tokenized_data

train_encodings = tokenize_data(X_train.tolist(), y_train.tolist())
val_encodings = tokenize_data(X_val.tolist(), y_val.tolist())
test_encodings = tokenizer(test_data['text'].tolist(), padding=True, truncation=True, max_length=128, return_tensors="pt")

class DisasterTweetsDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        return {key: val[idx] for key, val in self.encodings.items()}

    def __len__(self):
        return len(self.encodings['input_ids'])

train_dataset = DisasterTweetsDataset(train_encodings)
val_dataset = DisasterTweetsDataset(val_encodings)

# Antrenarea modelului DistilBERT
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy="epoch"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

trainer.train()

# Evaluarea modelului pe setul de validare
val_results = trainer.evaluate()

print(f"Acuratețea pe setul de validare: {val_results['eval_accuracy'] * 100:.2f}%")

# Generarea predicțiilor pentru setul de testare
predictions = trainer.predict(test_encodings).predictions
test_predictions = np.argmax(predictions, axis=1)

# Crearea DataFrame-ului pentru predicții
submission = pd.DataFrame({'id': test_data['id'], 'target': test_predictions})

# Salvarea predicțiilor într-un fișier CSV
submission.to_csv('submission.csv', index=False)

files.download('submission.csv')