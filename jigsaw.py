# -*- coding: utf-8 -*-
"""Jigsaw.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gBzRda2tI2jzIGzC6AyICNKbJ-mgP99N
"""

from google.colab import files
files.upload()

import os
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle competitions download -c jigsaw-multilingual-toxic-comment-classification
!unzip jigsaw-multilingual-toxic-comment-classification.zip -d jigsaw

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# Load the datasets
train_df = pd.read_csv('jigsaw/jigsaw-toxic-comment-train.csv')
unintended_bias_train_df = pd.read_csv('jigsaw/jigsaw-unintended-bias-train.csv')
validation_df = pd.read_csv('jigsaw/validation.csv')
test_df = pd.read_csv('jigsaw/test.csv')
sample_submission = pd.read_csv('jigsaw/sample_submission.csv')

# Preprocessing text data
def preprocess_text(text):
    text = text.lower()
    text = text.replace('\n', ' ')
    return text

train_df['comment_text'] = train_df['comment_text'].apply(preprocess_text)
validation_df['comment_text'] = validation_df['comment_text'].apply(preprocess_text)
test_df['content'] = test_df['content'].apply(preprocess_text)

# Tokenize the text data
tokenizer = Tokenizer(num_words=50000)
tokenizer.fit_on_texts(train_df['comment_text'])

X_train = tokenizer.texts_to_sequences(train_df['comment_text'])
X_val = tokenizer.texts_to_sequences(validation_df['comment_text'])
X_test = tokenizer.texts_to_sequences(test_df['content'])

# Pad sequences to ensure uniform input size
max_length = 128
X_train = pad_sequences(X_train, maxlen=max_length, padding='post')
X_val = pad_sequences(X_val, maxlen=max_length, padding='post')
X_test = pad_sequences(X_test, maxlen=max_length, padding='post')

y_train = train_df['toxic'].values
y_val = validation_df['toxic'].values

embedding_dim = 100

model = Sequential([
    Embedding(input_dim=50000, output_dim=embedding_dim, input_length=max_length),
    Bidirectional(LSTM(64, return_sequences=True)),
    Dropout(0.5),
    Bidirectional(LSTM(32)),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Adding early stopping and model checkpoint
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min')

history = model.fit(
    X_train, y_train,
    epochs=10,
    batch_size=128,
    validation_data=(X_val, y_val),
    #callbacks=[early_stopping, model_checkpoint]
)

# Load the best model
model.load_weights('best_model.h5')

# Make predictions on the test set
test_predictions = model.predict(X_test)
test_predictions = (test_predictions > 0.5).astype(int)

# Prepare the submission file
submission = pd.DataFrame({'id': test_df['id'], 'toxic': test_predictions.flatten()})
submission.to_csv('submission.csv', index=False)

files.download('submission.csv')