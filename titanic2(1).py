# -*- coding: utf-8 -*-
"""Titanic2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FTk17eeaZyMtznkQpmTKbANvI5OeMMTm
"""

from google.colab import files
files.upload()

import os
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle competitions download -c titanic
!unzip titanic.zip -d titanic

# Importăm librăriile necesare
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Încărcăm datele
train_df = pd.read_csv('titanic/train.csv')
test_df = pd.read_csv('titanic/test.csv')

# Feature Engineering: Extragem titlurile din nume
train_df['Title'] = train_df['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)
test_df['Title'] = test_df['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)

# Grupăm titlurile rare într-o categorie 'Rare'
train_df['Title'] = train_df['Title'].replace(['Lady', 'Countess', 'Capt', 'Col',
                                               'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')
train_df['Title'] = train_df['Title'].replace('Mlle', 'Miss')
train_df['Title'] = train_df['Title'].replace('Ms', 'Miss')
train_df['Title'] = train_df['Title'].replace('Mme', 'Mrs')

test_df['Title'] = test_df['Title'].replace(['Lady', 'Countess', 'Capt', 'Col',
                                             'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')
test_df['Title'] = test_df['Title'].replace('Mlle', 'Miss')
test_df['Title'] = test_df['Title'].replace('Ms', 'Miss')
test_df['Title'] = test_df['Title'].replace('Mme', 'Mrs')

# Mapăm titlurile pe numere
title_mapping = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Rare': 5}
train_df['Title'] = train_df['Title'].map(title_mapping)
test_df['Title'] = test_df['Title'].map(title_mapping)
train_df['Title'] = train_df['Title'].fillna(0)
test_df['Title'] = test_df['Title'].fillna(0)

# Creăm caracteristica 'IsAlone'
train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1
test_df['FamilySize'] = test_df['SibSp'] + test_df['Parch'] + 1
train_df['IsAlone'] = 1  # inițial presupunem că pasagerul este singur
train_df['IsAlone'].loc[train_df['FamilySize'] > 1] = 0
test_df['IsAlone'] = 1
test_df['IsAlone'].loc[test_df['FamilySize'] > 1] = 0

# Preprocesăm datele: eliminăm coloanele inutile și umplem valorile lipsă
train_df = train_df.drop(['Ticket', 'Cabin', 'Name', 'PassengerId'], axis=1)
test_df = test_df.drop(['Ticket', 'Cabin', 'Name'], axis=1)

# Umplem valorile lipsă pentru 'Age' și 'Fare'
train_df['Age'].fillna(train_df['Age'].median(), inplace=True)
test_df['Age'].fillna(test_df['Age'].median(), inplace=True)
train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace=True)
test_df['Embarked'].fillna(test_df['Embarked'].mode()[0], inplace=True)
test_df['Fare'].fillna(test_df['Fare'].median(), inplace=True)

# Convertim datele categorice în numerice
train_df['Sex'] = train_df['Sex'].map({'male': 0, 'female': 1})
test_df['Sex'] = test_df['Sex'].map({'male': 0, 'female': 1})
train_df['Embarked'] = train_df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})
test_df['Embarked'] = test_df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})

# Standardizăm datele numerice
scaler = StandardScaler()
numerical_features = ['Age', 'Fare', 'FamilySize']
train_df[numerical_features] = scaler.fit_transform(train_df[numerical_features])
test_df[numerical_features] = scaler.transform(test_df[numerical_features])

# Separăm variabilele de intrare și ieșire
X_train = train_df.drop('Survived', axis=1)
y_train = train_df['Survived']
X_test = test_df.drop('PassengerId', axis=1)

# Antrenăm mai multe modele și folosim GridSearchCV pentru optimizarea hiperparametrilor
# Random Forest
rf = RandomForestClassifier(random_state=42)
rf_params = {'n_estimators': [100, 200, 300], 'max_depth': [None, 5, 10]}
rf_grid = GridSearchCV(rf, rf_params, cv=5, scoring='accuracy')
rf_grid.fit(X_train, y_train)

# Gradient Boosting
gb = GradientBoostingClassifier(random_state=42)
gb_params = {'n_estimators': [100, 200], 'learning_rate': [0.1, 0.05], 'max_depth': [3, 5]}
gb_grid = GridSearchCV(gb, gb_params, cv=5, scoring='accuracy')
gb_grid.fit(X_train, y_train)

# Logistic Regression
lr = LogisticRegression(random_state=42, max_iter=1000)
lr_params = {'C': [0.1, 1, 10], 'solver': ['liblinear']}
lr_grid = GridSearchCV(lr, lr_params, cv=5, scoring='accuracy')
lr_grid.fit(X_train, y_train)

# Comparăm performanța modelelor
models = {'Random Forest': rf_grid, 'Gradient Boosting': gb_grid, 'Logistic Regression': lr_grid}
for model_name, model in models.items():
    print(f"{model_name}: Best score = {model.best_score_}, Best params = {model.best_params_}")

# Facem predicții folosind cel mai bun model
best_model = rf_grid.best_estimator_  # sau gb_grid.best_estimator_ sau lr_grid.best_estimator_
predictions = best_model.predict(X_test)

# Salvăm predicțiile în fișierul submission.csv
output = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived': predictions})
output.to_csv('submission.csv', index=False)
print("Submission saved!")
files.download('submission.csv')