# -*- coding: utf-8 -*-
"""PytorchIMDB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sGFLpxQ850CV013nR0wmpzoWDVaAdNj_
"""

!pip install transformers datasets torch nltk nlpaug accelerate
import random
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import nlpaug.augmenter.word as naw
import nltk
from nltk.corpus import wordnet

# Descărcare resurse nltk necesare
nltk.download('wordnet')

# Instalează librăriile necesare
!pip install transformers datasets torch
!pip install requests==2.31.0

# Funcție pentru a găsi sinonime
def get_synonyms(word):
    synonyms = set()
    for syn in wordnet.synsets(word):
        for lemma in syn.lemmas():
            synonyms.add(lemma.name())
    if word in synonyms:
        synonyms.remove(word)
    return list(synonyms)

# Funcție de augmentare prin înlocuirea cu sinonime
def synonym_replacement(text, n=1):
    words = text.split()
    new_words = words.copy()
    random_word_list = list(set(words))
    random.shuffle(random_word_list)
    num_replaced = 0
    for random_word in random_word_list:
        synonyms = get_synonyms(random_word)
        if len(synonyms) >= 1:
            synonym = random.choice(synonyms)
            new_words = [synonym if word == random_word else word for word in new_words]
            num_replaced += 1
        if num_replaced >= n:
            break
    return ' '.join(new_words)

# Încărcare tokenizer pentru BERT
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Funcție de tokenizare cu augmentare
def tokenize_and_augment(examples):
    augmented_texts = [synonym_replacement(text, n=2) for text in examples["text"]]
    return tokenizer(augmented_texts, padding="max_length", truncation=True)

# Încărcare dataset IMDb
dataset = load_dataset("imdb")

# Aplicarea tokenizării și augmentării pe setul de date
tokenized_datasets = dataset.map(tokenize_and_augment, batched=True)

# Convertim toate câmpurile la tensor și ne asigurăm că includem etichetele
def format_dataset(dataset):
    dataset = dataset.remove_columns(["text"])
    dataset = dataset.rename_column("label", "labels")
    dataset.set_format("torch")
    return dataset

train_dataset = format_dataset(tokenized_datasets["train"].shuffle(seed=42).select(range(1000)))
test_dataset = format_dataset(tokenized_datasets["test"].shuffle(seed=42).select(range(1000)))

!pip install accelerate -U

# Încărcare model preantrenat
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

# Definirea argumentelor de antrenament
training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=3e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=5,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    logging_dir='./logs',
    logging_steps=10,
)

# Definirea trainer-ului
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
)

trainer.train()